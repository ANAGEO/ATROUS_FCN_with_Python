{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN TRAINING SAMPLES PREPERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"adapted from the coded developed in Bergado, J. R. A., Persello, C., & Gevaert, C. (2016). \\n    A deep learning approach to the classification of sub-decimeter resolution aerial images.\\n    In IEEE International Geoscience and Remote Sensing Symposium (pp. 1516\\xe2\\x80\\x931519). Beijing.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##code for preparing the training data.\n",
    "\"\"\"\"adapted from the original code developed in Bergado, J. R. A., Persello, C., & Gevaert, C. (2016). \n",
    "    A deep learning approach to the classification of sub-decimeter resolution aerial images.\n",
    "    In IEEE International Geoscience and Remote Sensing Symposium (pp. 1516â€“1519). Beijing.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##importing libraries and dependencies\n",
    "from __future__ import division, print_function\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## defining the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions for loading the images and converting them to arrays\n",
    "\n",
    "def img_to_array(*images):\n",
    "    \"\"\"Convert an image or list of images to numpy arrays.\n",
    "\n",
    "    Keyword arguments:\n",
    "    *images -- list containing the images to be converted\n",
    "    \"\"\"\n",
    "    imgarrays = []\n",
    "    i = 0\n",
    "    for img in images:\n",
    "        arr = gtiff_to_array(img)\n",
    "        imgarrays.append(arr)\n",
    "    return imgarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the geotiff to a numpy array\n",
    "\n",
    "def gtiff_to_array(imgfname):\n",
    "    \"\"\"Transform a geotiff to numpy array.\n",
    "\n",
    "    Keyword arguments:\n",
    "    imgfnames -- filename of image to convert\n",
    "    \"\"\"\n",
    "    ds = gdal.Open(imgfname)\n",
    "    for band in range(ds.RasterCount):\n",
    "        band += 1\n",
    "        if band == 1:\n",
    "            arr = np.array(ds.GetRasterBand(band).ReadAsArray())\n",
    "            arr = np.expand_dims(arr, axis=2)\n",
    "        else:\n",
    "            concat = np.array(ds.GetRasterBand(band).ReadAsArray())\n",
    "            concat = np.expand_dims(concat, axis=2)\n",
    "            arr = np.concatenate((arr,\n",
    "                                  concat),\n",
    "                                 axis=2)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the top array\n",
    "\n",
    "def norm_data(data):\n",
    "    data = data.astype(float)\n",
    "    l_max = np.amax(data)\n",
    "    l_min = np.amin(data)\n",
    "    #l_max = 2424\n",
    "    #l_min = 7\n",
    "    data_norm = (data - l_min)/(l_max - l_min)\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing the null values and assigning them to 0\"\n",
    "\n",
    "def nulltozero(arr):\n",
    "    arrcopy = np.copy(arr)\n",
    "    low_values_flags = arrcopy < 0 #0.7,0\n",
    "    arrcopy[low_values_flags]=0\n",
    "    return arrcopy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampling an array based on the class frequency\n",
    "def sample_idx(arr):\n",
    "    \"\"\"Randomly sample an array stratified based on frequency.\n",
    "\n",
    "    Keyword arguments:\n",
    "    arr -- the array being sampled\n",
    "    cratios -- the representative fractions of each classes\n",
    "    n -- total number of samples (default 1000)\n",
    "    \n",
    "    i = 0\n",
    "    arr_copy = np.copy(arr)\n",
    "    idxarray = np.zeros(shape=(0, 2), dtype=np.int16)\n",
    "    nc = np.array(np.where(arr_copy >0)).T.shape[0]\n",
    "    #sampleidx = 0\n",
    "    #nc = np.array(np.where(arr_copy == i+1 or arr_copy == i+2)).T.shape[0]\n",
    "    #nc = np.array(np.logical_or(arr_copy == i+1, arr_copy == i+2)).T.shape[0]\n",
    "    #find how to automatically select the samples\n",
    "    randidx = np.random.choice(range(nc),\n",
    "\t\t\t\t\t\t\t   size=nc, replace=False)\n",
    "    #print()\n",
    "\t#randidx.dtype = np.int32\n",
    "    #idxarray = np.concatenate((idxarray,\n",
    "\t#\t\t\t\t\t\t   np.array(np.where(arr_copy == i+1)).T\n",
    "\t#\t\t\t\t\t\t   [randidx, :]),\n",
    "\t#\t\t\t\t\t\t  axis=0)\n",
    "    #idxarray = np.array(np.where(arr_copy == i+1)).T[randidx, :]\n",
    "    randidx=randidx.astype(np.int32)\n",
    "    idxarray = np.array(np.where(arr_copy > 0)).T[randidx, :]\n",
    "\t#sampleidx += csamples\n",
    "\n",
    "    del arr_copy\n",
    "    return idxarray\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampling an array\n",
    "def sample_idx(arr):\n",
    "    \"\"\"Randomly sample an array\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    arr_copy = np.copy(arr)\n",
    "    idxarray = np.zeros(shape=(0, 2), dtype=np.int16)\n",
    "    nc = np.array(np.where(arr_copy >0)).T.shape[0]\n",
    "    #find how to automatically select the samples\n",
    "    randidx = np.random.choice(range(nc),\n",
    "\t\t\t\t\t\t\t   size=nc, replace=False)\n",
    "    randidx=randidx.astype(np.int32)\n",
    "    idxarray = np.array(np.where(arr_copy > 0)).T[randidx, :]\n",
    "\n",
    "    del arr_copy\n",
    "    return idxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"find a way of skipping this step\"\n",
    "# not call this function\n",
    "#build the training and test sets\n",
    "\n",
    "def split_samples(idxarray, proportion=0.67):\n",
    "    \"\"\"Split indices into two sets.\n",
    "\n",
    "    Keyword arguments:\n",
    "    idxarray -- the array of indices to be split\n",
    "    proportion -- the proportion of split\n",
    "    \"\"\"\n",
    "    nsamples = idxarray.shape[0]\n",
    "    nsub1 = int(round(proportion*nsamples))\n",
    "    randidx = np.random.choice(range(nsamples), size=nsub1, replace=False)\n",
    "\t#setting randidx data type to integer, the other option is to be a boolean\n",
    "    randidx.dtype = np.int32\n",
    "    idxsub1 = idxarray[randidx, :]\n",
    "    randidx = list(set(range(nsamples)) - set(randidx))\n",
    "    idxsub2 = idxarray[randidx, :]\n",
    "\n",
    "    return (idxsub1, idxsub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"define the function for accumulating samples\"\n",
    "def accum_samples_cnn(accumarray, fset, idxarray, psize, sparse=True,\n",
    "                      ignoreedge=125):\n",
    "    \"\"\"Accumulate samples from different fset for a cnn model.\n",
    "\n",
    "    Keyword arguments:\n",
    "    accumarray -- the array containing accumulated samples\n",
    "    fset -- the tuple containg feature set arrays\n",
    "    idxarray -- the array containing indices of samples\n",
    "    psize - the size of the context patch\n",
    "    sparse -- a flag used for the type of sampling. Set to False to\n",
    "    sample whole image (by batch) and hence using ntrain and ntest.\n",
    "    Returns the indices when sparse is set to False.\n",
    "    \"\"\"\n",
    "    n = idxarray.shape[0]\n",
    "    include = int(math.floor(psize/2))\n",
    "    exclude = include\n",
    "\n",
    "    # limit sampling using the max patch size for ignoring pixels\n",
    "    if ignoreedge:\n",
    "        exclude = int(math.floor(ignoreedge/2))\n",
    "    # gather patches without zero-padding\n",
    "    nonedgesamples = []\n",
    "    f0 = fset[0]\n",
    "    print(\"f0 shape\",f0.shape)\n",
    "    nonedgeidx = np.asarray([rc\n",
    "                             for rc in idxarray if\n",
    "                             ((rc[0] > exclude - 1) and\n",
    "                              (rc[1] > exclude - 1) and\n",
    "                              (rc[0] < f0.shape[0] - exclude) and\n",
    "                              (rc[1] < f0.shape[1] - exclude))\n",
    "                             ])\n",
    "\n",
    "    for f in fset:\n",
    "        nonedgesamples.append(np.asarray([f[rc[0]-include:rc[0]+include+1,\n",
    "                                            rc[1]-include:rc[1]+include+1, ]\n",
    "                                          for rc in nonedgeidx]))\n",
    "    #print(\"nonedgesamples\",nonedgesamples)\n",
    "    # gather patches with zero-padding\n",
    "    edgeidx = np.asarray([rc\n",
    "                          for rc in idxarray if\n",
    "                          ((rc[0] <= exclude - 1) or\n",
    "                           (rc[1] <= exclude - 1) or\n",
    "                           (rc[0] >= f0.shape[0] - exclude) or\n",
    "                           (rc[1] >= f0.shape[1] - exclude))\n",
    "                          ])\n",
    "    #jr case,ncols evaluates to 5\n",
    "\t#my case, f0 has 4 bands ,therefore evaluates to 6. so i will add 1 instead of 2\n",
    "\t#ncols = f0.shape[2] + 2\n",
    "    #ncols = f0.shape[2] + 1 ncols is the numbero bands plus 1\n",
    "    ncols=4\n",
    "    print(\"ncols\",ncols)\n",
    "    if edgeidx.shape[0] != 0:\n",
    "        edgesamples = build_edge_samples(edgeidx, fset, include)\n",
    "        edgefset = np.concatenate(edgesamples, axis=3)\n",
    "    else:\n",
    "        edgefset = np.zeros(shape=(0, psize, psize, ncols))\n",
    "\n",
    "    # gather all features\n",
    "    # check for small batches consisting of all edge pixels\n",
    "\tprint(\"noneedgesamples shape\",len(nonedgesamples))\n",
    "    print(\"nonedgesamples[0].shape[0]\",nonedgesamples[0].shape[0])\n",
    "    #print(\"noneedgesamples \",nonedgesamples)\n",
    "    \"\"\"if nonedgesamples[0].shape[0] != 0:\n",
    "        nonedgefset = np.concatenate((nonedgesamples[0],\n",
    "                                      nonedgesamples[1],\n",
    "                                      np.expand_dims(nonedgesamples[2],\n",
    "                                                     axis=3)),\n",
    "                                     axis=3)\"\"\"\n",
    "    #print(\"nonedgesamples\",nonedgesamples[0])\n",
    "    if nonedgesamples[0].shape[0] != 0:\n",
    "        nonedgefset = np.concatenate((nonedgesamples[0],\n",
    "                                      np.expand_dims(nonedgesamples[1], \n",
    "                                      \n",
    "                                                     axis=3)),\n",
    "                                     axis=3)\n",
    "\t\"\"\"if nonedgesamples[0].shape[0] != 0:\n",
    "        print(\"this is so far true\")\n",
    "        nonedgefset = np.concatenate((nonedgesamples[0],\n",
    "                                      np.expand_dims(nonedgesamples[1],axis=3)),\n",
    "                                     axis=3)\"\"\"\n",
    "    else:\n",
    "        print(\"else statement executed\")\n",
    "        nonedgefset = np.zeros(shape=(0, psize, psize, ncols))\n",
    "        #print(\"nonedgefset shape\",nonedgefset.shape)\n",
    "    #print nonedgefset\n",
    "    #print(\"nonedgefset\",nonedgefset)\n",
    "\t#print nonedgefset shape\n",
    "    print(\"nonedgefset shape\",nonedgefset.shape)\n",
    "\t# gather all samples\n",
    "    if ignoreedge:\n",
    "        accumarray = np.concatenate((accumarray, nonedgefset), axis=0)\n",
    "        idxarray = nonedgeidx\n",
    "\t\t\n",
    "    else:\n",
    "        accumarray = np.concatenate((accumarray, nonedgefset, edgefset),\n",
    "                                    axis=0)\n",
    "        if nonedgeidx.shape[0] == 0:\n",
    "            nonedgeidx = nonedgeidx.reshape(0, 2)\n",
    "        if edgeidx.shape[0] == 0:\n",
    "            edgeidx = edgeidx.reshape(0, 2)\n",
    "        idxarray = np.concatenate((nonedgeidx, edgeidx), axis=0)\n",
    "\n",
    "    return (accumarray, idxarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_edge_samples(idxarray, fset, pad):\n",
    "    \"\"\"Pad zeros to patches centered in edge and near-edge pixels.\n",
    "\n",
    "    Keyword arguments:\n",
    "    idxarray -- the index array of edge and near-edge pixels\n",
    "    fset -- the tuple containg feature set arrays\n",
    "    pad -- the number of zeros to pad on each side of the image\n",
    "    \"\"\"\n",
    "    paddedfset = []\n",
    "    for f in fset:\n",
    "        if f.ndim != 2:\n",
    "            padded = np.zeros((f.shape[0] + 2*pad,\n",
    "                               f.shape[1] + 2*pad, f.shape[2]),\n",
    "                              dtype=f.dtype)\n",
    "            padded[pad:f.shape[0]+pad,\n",
    "                   pad:f.shape[1]+pad, ] = f\n",
    "        else:\n",
    "            padded = np.zeros((f.shape[0] + 2*pad,\n",
    "                               f.shape[1] + 2*pad, 1),\n",
    "                              dtype=f.dtype)\n",
    "            padded[pad:f.shape[0]+pad,\n",
    "                   pad:f.shape[1]+pad, ] = f.reshape((f.shape[0],\n",
    "                                                      f.shape[1],\n",
    "                                                      1))\n",
    "        paddedfset.append(padded)\n",
    "    edgesamples = []\n",
    "    for f in paddedfset:\n",
    "        edgesamples.append(np.asarray([f[rc[0]:rc[0]+2*pad+1,\n",
    "                                         rc[1]:rc[1]+2*pad+1, ]\n",
    "                                       for rc in idxarray\n",
    "                                       ]))\n",
    "\n",
    "    return edgesamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_samplesets(alltrainset, fname,idfname,\n",
    "                    nc=2, tp=0.5, labels=\"central\"):\n",
    "    \"\"\"Save the splitted set of samples in an hdf5 file.\n",
    "\n",
    "    alltrainset -- the array containing all the training samples,\n",
    "    this will be split into training and validation sets\n",
    "    alltestset -- the array containing all test samples\n",
    "    fname -- the filename (and directory) where to write the hdf5\n",
    "    nc -- the number of classes\n",
    "    tp -- the proportional split of training samples\n",
    "    from alltrainset\n",
    "    \"\"\"\n",
    "    nf = alltrainset.shape[1] - 1\n",
    "\n",
    "    # separate X's (features) and y's (labels)\n",
    "    (X_train, y_train) = (alltrainset[:, 0:nf, ],\n",
    "                          alltrainset[:, nf, ])\n",
    "    #(X_test, y_test) = (alltestset[:, 0:nf, ],\n",
    "    #                    alltestset[:, nf, ])\n",
    "    # take only the label of the central pixel\n",
    "    #span = int(math.floor(y_train.shape[2]/2))\n",
    "    #if labels==\"central\":\n",
    "    #    y_train = y_train[:, span, span]\n",
    "    #    y_test = y_test[:, span, span]\n",
    "\n",
    "    X_train = X_train.astype(\"float32\")\n",
    "    #X_test = X_test.astype(\"float32\")\n",
    "    # convert class vectors to binary class matrices\n",
    "    #if labels==\"central\":\n",
    "    #    Y_train = np_utils.to_categorical(y_train, nc)\n",
    "    #    Y_test = np_utils.to_categorical(y_test, nc)\n",
    "    #elif labels==\"full\":\n",
    "    #    Y_train = to_categorical_4d(y_train, nc)\n",
    "    #    Y_test = to_categorical_4d(y_test, nc)\n",
    "                \n",
    "\n",
    "    # split trainset into training and validation\n",
    "    origidx = np.arange(X_train.shape[0]).reshape(X_train.shape[0], 1)\n",
    "    trainidx, validx = split_samples(origidx,\n",
    "                                     tp)\n",
    "    trainidx = trainidx.reshape(trainidx.shape[0])\n",
    "    validx = validx.reshape(validx.shape[0])\n",
    "\t\n",
    "\t#save the indices of the training sets\n",
    "    \n",
    "    with h5py.File(idfname,mode = \"w\") as f:\n",
    "\t    f[\"trainidx\"]=trainidx\n",
    "\t    f[\"validx\"] =validx\n",
    "    \n",
    "\t\n",
    "\t\n",
    "    X_val = X_train[validx, :]\n",
    "    #Y_val = Y_train[validx, :]\n",
    "    y_val = y_train[validx]\n",
    "    X_train = X_train[trainidx, :]\n",
    "    #Y_train = Y_train[trainidx, :]\n",
    "    y_train = y_train[trainidx]\n",
    "\n",
    "    y_train = np.expand_dims(y_train, axis=1)\n",
    "    y_val = np.expand_dims(y_val, axis=1)\n",
    "    #y_test = np.expand_dims(y_test, axis=1)\n",
    "\n",
    "    with h5py.File(fname, mode=\"w\") as f:\n",
    "        f[\"X_train\"] = X_train\n",
    "        f[\"X_val\"] = X_val\n",
    "        #f[\"X_test\"] = X_test\n",
    "        #f[\"Y_train\"] = Y_train\n",
    "        #f[\"Y_val\"] = Y_val\n",
    "        #f[\"Y_test\"] = Y_test\n",
    "        #f[\"y_test\"] = y_test\n",
    "        f[\"y_train\"] = y_train\n",
    "        f[\"y_val\"] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##paths\n",
    "\n",
    "\"paths to the new dataset\"\n",
    "rootfolder = \"H:/BUKAVU-2018/PRELIM_CNN/\"\n",
    "raster_data_path = \"H:/tiles/grayscale/tile2cnn/\"\n",
    "#tile2=raster_data_path+\"tile2_v2_gray.tif\"\n",
    "#tile2 = \"H:/tiles/grayscale/tile2_v4_gray_n.tif\"\n",
    "tile2 = \"H:/goma_train_seg/TRAINING_TILE/TILE2_clip.tif\"\n",
    "#tile2 = \"H:/goma_train_seg/TRAINING_TILE/TILE2_clip_geobia2.tif\"\n",
    "\n",
    "#tile2_dsm = raster_data_path+\"tile2_ndsm_50_clip.tif\"\n",
    "\n",
    "gts_paths = \"H:/BUKAVU-2018/PRELIM_CNN/GTS_IMAGES/\"\n",
    "training_pts=gts_paths+\"training_pts_ndsm50.tiff\"\n",
    "training_vector=gts_paths+\"training_vector_ndsm50.tiff\"\n",
    "\n",
    "#classified_raster = \"H:/goma_train_seg/classified_raster/TILE2_clip_geobia2.tif\"\n",
    "classified_raster = \"H:/goma_train_seg/TRAINING_TILE/TILE_clip_geobia2.tif\"\n",
    "\n",
    "tutorialdir = \"H:/tutorial_fcn/\"\n",
    "smpldir= tutorialdir  + \"samples/samples_wfcn_v1.hdf5\"\n",
    "inddir = tutorialdir  + \"samples/samples_indir_wfcn_v1.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tile2_cr is the classified raster\n",
    "#TO DO -find a way to automatically mask the computation region\n",
    "#http://karthur.org/2015/clipping-rasters-in-python.html\n",
    "tile2_top,tile2_points,tile2_poly,tile2_cr = img_to_array(tile2,training_pts,training_vector,classified_raster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9320L, 7744L, 3L)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tile2_top.shape\n",
    "#tile2_poly.shape\n",
    "#tile2_points.shape\n",
    "#tile2_cr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport gdal\\nfrom gdalconst import GA_ReadOnly\\ndata = gdal.Open(training_pts, GA_ReadOnly)\\ngeoTransform = data.GetGeoTransform()\\nminx = geoTransform[0]\\nmaxy = geoTransform[3]\\nmaxx = minx + geoTransform[1] * data.RasterXSize\\nminy = maxy + geoTransform[5] * data.RasterYSize\\n'gdal_translate -projwin ' + ' '.join([str(x) for x in [minx, maxy, maxx, miny]]) + ' -of GTiff img_orig.tif img_out.tif', shell=True)\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import gdal\n",
    "from gdalconst import GA_ReadOnly\n",
    "data = gdal.Open(training_pts, GA_ReadOnly)\n",
    "geoTransform = data.GetGeoTransform()\n",
    "minx = geoTransform[0]\n",
    "maxy = geoTransform[3]\n",
    "maxx = minx + geoTransform[1] * data.RasterXSize\n",
    "miny = maxy + geoTransform[5] * data.RasterYSize\n",
    "'gdal_translate -projwin ' + ' '.join([str(x) for x in [minx, maxy, maxx, miny]]) + ' -of GTiff img_orig.tif img_out.tif', shell=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"normalizing the input image\"\n",
    "tile2_top = nulltozero(tile2_top)\n",
    "tile2_top=tile2_top/np.amax(tile2_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"reshaping the gts\"\n",
    "tile2_poly = np.reshape(tile2_poly,(tile2_poly.shape[0],tile2_poly.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"create an idx array by sampling from the tile2_points\"\n",
    "\n",
    "idx_tile2= sample_idx(tile2_points[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2987L, 2L)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#idx_tile2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#proportion = 0.995\n",
    "\"create the tr and ts idx\"\n",
    "#(tr_idx_tile2, ts_idx_tile2)= split_samples(idx_tile2,proportion)\n",
    "# defining the ids of the training samples\n",
    "tr_idx_tile2 = idx_tile2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f0 shape (9320L, 7744L, 3L)\n",
      "ncols 4\n",
      "nonedgesamples[0].shape[0] 2954\n",
      "nonedgefset shape (2954L, 33L, 33L, 4L)\n"
     ]
    }
   ],
   "source": [
    "psize = 33\n",
    "#nbands = top_cl_1.shape[2] + 1\n",
    "ignoreedge = 0\n",
    "\n",
    "nbands = tile2_top.shape[2] + 1\n",
    "trainset = np.zeros(shape=(0, psize, psize, nbands))                \n",
    "#testset = np.zeros(shape=(0, psize, psize, nbands))\n",
    "\n",
    "\"accumulate samples for tile 2\"\n",
    "(trainset,trainidx) = accum_samples_cnn(trainset,(tile2_top,tile2_poly),\n",
    "\t\t\t\t\t\t\t\t\t\t\t   tr_idx_tile2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t   psize, ignoreedge=ignoreedge)\n",
    "#(testset,testidx) = accum_samples_cnn(testset,(tile2_top,tile2_poly),\n",
    "#\t\t\t\t\t\t\t\t\t\t\t  ts_idx_tile2,\n",
    "#\t\t\t\t\t\t\t\t\t\t\t  psize, ignoreedge=ignoreedge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = nbands-1\t\t\t\t\t\t\t\t\t\t\t \n",
    "#reshaping the trainset and the testset\n",
    "trainset = np.swapaxes(trainset, 1, 3)\n",
    "trainset = np.swapaxes(trainset, 2, 3)\n",
    "#dataset = trainset\n",
    "trainset[:, nf, :, :] = trainset[:, nf, :, :] - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nf = nbands-1\t\t\t\t\t\t\t\t\t\t\t \n",
    "#reshaping the trainset and the testset\n",
    "#trainset = np.swapaxes(trainset, 1, 3)\n",
    "#trainset = np.swapaxes(trainset, 2, 3)\n",
    "#dataset = trainset\n",
    "#trainset[:, nf, :, :] = trainset[:, nf, :, :] - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2987L, 4L, 33L, 33L)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of the trainset\n",
    "trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84313726,  0.85882354,  0.86666667, ...,  0.80000001,\n",
       "         0.7764706 ,  0.80784315],\n",
       "       [ 0.88627452,  0.89019608,  0.88627452, ...,  0.80392158,\n",
       "         0.7647059 ,  0.80000001],\n",
       "       [ 0.89019608,  0.91764706,  0.84705883, ...,  0.81176472,\n",
       "         0.81960785,  0.8392157 ],\n",
       "       ..., \n",
       "       [ 0.73333335,  0.73725492,  0.71372551, ...,  0.94901961,\n",
       "         0.98039216,  1.        ],\n",
       "       [ 0.71372551,  0.74901962,  0.70588237, ...,  0.93725491,\n",
       "         0.96078432,  0.98431373],\n",
       "       [ 0.69411767,  0.73333335,  0.67843139, ...,  0.9254902 ,\n",
       "         0.94117647,  0.95686275]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smpldir= rootfolder + \"samples/samples_weakly_geobia2.hdf5\"\n",
    "#inddir = rootfolder + \"samples/samples_indir_weakly_geobia2.hdf5\"\n",
    "\n",
    "trainproportion = 0.8\n",
    "alltrainset = trainset\n",
    "#alltestset = testset\n",
    "nc=5\n",
    "save_samplesets(alltrainset=alltrainset,\n",
    "                   #alltestset=alltestset,\n",
    "                   fname=smpldir,idfname = inddir,\n",
    "                   nc=nc, tp=trainproportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
